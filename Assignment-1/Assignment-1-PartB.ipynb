{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-1 (KNN Classifier)--PartB\n",
    "## Breast cancer wisconsin dataset\n",
    "In this assignment, we will build a KNN classifier that takes an features as as input and outputs a label 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The breast cancer dataset contains 569 samples with 30 real, positive features (including cancer mass attributes like mean radius, mean texture, mean perimeter, et cetera). Of the samples, 212 are labeled “malignant” and 357 are labeled “benign”. \n",
    "You can find more details in: https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip, os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mode\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "## Load the training set\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number samples =  17070\n",
      "print the number of features per sample\n",
      "0     569\n",
      "1     569\n",
      "2     569\n",
      "3     569\n",
      "4     569\n",
      "5     569\n",
      "6     569\n",
      "7     569\n",
      "8     569\n",
      "9     569\n",
      "10    569\n",
      "11    569\n",
      "12    569\n",
      "13    569\n",
      "14    569\n",
      "15    569\n",
      "16    569\n",
      "17    569\n",
      "18    569\n",
      "19    569\n",
      "20    569\n",
      "21    569\n",
      "22    569\n",
      "23    569\n",
      "24    569\n",
      "25    569\n",
      "26    569\n",
      "27    569\n",
      "28    569\n",
      "29    569\n",
      "dtype: int64\n",
      "\n",
      "Total number of classes =  [0 1]\n",
      "\n",
      "Training set distribution:\n",
      "{0: 212, 1: 357}\n"
     ]
    }
   ],
   "source": [
    "## print some statistics on the dataset\n",
    "### TODO YOUR CODE ###\n",
    "df = pd.DataFrame(X)\n",
    "dfy = pd.DataFrame(y)\n",
    "# print(\"Total number of samples: \", )\n",
    "print(\"Total number samples = \", X.shape[1]*X.shape[0])\n",
    "\n",
    "# print the number of features per sample\n",
    "print(\"print the number of features per sample\")\n",
    "print(df.count())\n",
    "\n",
    "# Total number of classes\n",
    "print(\"\\nTotal number of classes = \" , dfy[0].unique())\n",
    "\n",
    "# print the number of samples in each class\n",
    "train_digits, train_counts = np.unique(y, return_counts=True)\n",
    "print(\"\\nTraining set distribution:\")\n",
    "print(dict(zip(train_digits, train_counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### TODO YOUR CODE ###\n",
    "trainx, testx, trainy, testy = train_test_split(X,y,test_size=0.30,random_state=777)\n",
    "testx, valx, testy, valy     = train_test_split(testx ,testy ,test_size=0.50,random_state=777)\n",
    "\n",
    "\n",
    "# Split the data (i.e. features and target) into 70% train, 15% validate, and 15% test; Use Random Seed 777\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Train data to Train and Validate Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor classification with L2 distance\n",
    "\n",
    "To compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is _Euclidean distance_: for two vectors $x, y \\in \\mathbb{R}^d$, their Euclidean distance is defined as \n",
    "$$\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.$$\n",
    "Often we omit the square root, and simply compute _squared Euclidean distance_:\n",
    "$$\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.$$\n",
    "For the purposes of nearest neighbor computations, the two are equivalent: for three vectors $x, y, z \\in \\mathbb{R}^d$, we have $\\|x - y\\| \\leq \\|x - z\\|$ if and only if $\\|x - y\\|^2 \\leq \\|x - z\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nearest neighbor classification with L2 distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, **NN_L2**, which takes as input the training data (`trainx` and `trainy`) and the test points (`evalx`) and predicts labels for these test points using 1-NN classification. These labels should be returned in a `numpy` array with one entry per test point. For **NN_L2**, the L2 norm should be used as the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(x1,x2):\n",
    "    return np.sum((x1-x2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_L2(trainx, trainy, evalx):\n",
    "    \n",
    "    nn_labels = []\n",
    "    \n",
    "    for points in evalx:\n",
    "\n",
    "        distances = [l2_distance(x,points) for x in trainx]\n",
    "        nn = np.argsort(distances)[:1]\n",
    "        \n",
    "        labels = trainy[nn]\n",
    "       \n",
    "        nn_labels.append(labels)\n",
    " \n",
    "\n",
    "    return nn_labels\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Nearest neighbor classification with L2 distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, **KNN_L2**, which takes as input the training data (`trainx` and `trainy`), the test points (`evalx`), and the value of **K** (integer) and predicts labels for these test points using K-NN classification. These labels should be returned in a `numpy` array with one entry per test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_L2(trainx, trainy, evalx, k):\n",
    "    knn_labels = []\n",
    "    \n",
    "    for points in evalx:\n",
    "\n",
    "        distances = [l2_distance(x,points) for x in trainx]\n",
    "        knn = np.argsort(distances)[:k]\n",
    "        \n",
    "        labels = trainy[knn]\n",
    "\n",
    "        vote = mode(labels)\n",
    "        knn_labels.append(vote)\n",
    "    \n",
    "\n",
    "    return knn_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nearest neighbor classification with L1 distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute nearest neighbors using the L1 distance (sometimes called *Manhattan Distance*).\n",
    "\n",
    "Write a function, **NN_L1**, which again takes as input the arrays `trainx`, `trainy`, and `evalx`, and predicts labels for the test points using 1-nearest neighbor classification. For **NN_L1**, the L1 distance metric should be used. As before, the predicted labels should be returned in a `numpy` array with one entry per test point.\n",
    "\n",
    "Notice that **NN_L1** and **NN_L2** may well produce different predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance(x1,x2):\n",
    "    return np.sum(np.abs(x1-x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_L1(trainx, trainy, evalx):\n",
    "    \n",
    "    nn_labels = []\n",
    "    \n",
    "    for points in evalx:\n",
    "\n",
    "        distances = [l1_distance(x,points) for x in trainx]\n",
    "        nn = np.argsort(distances)[:1]\n",
    "        labels = trainy[nn]\n",
    "       \n",
    "        nn_labels.append(labels)\n",
    " \n",
    "\n",
    "    return nn_labels\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Nearest neighbor classification with L1 distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, **KNN_L1**, which takes as input the training data (`trainx` and `trainy`), the test points (`evalx`), and the value of **K** (integer) and predicts labels for these test points using K-NN classification and L1 distance metric. These labels should be returned in a `numpy` array with one entry per test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_L1(trainx, trainy, evalx, k):\n",
    "    \n",
    "    knn_labels = []\n",
    "    \n",
    "    for points in evalx:\n",
    "\n",
    "        distances = [l1_distance(x,points) for x in trainx]\n",
    "        knn = np.argsort(distances)[:k]\n",
    "        \n",
    "        labels = trainy[knn]\n",
    "\n",
    "        vote = mode(labels)\n",
    "\n",
    "        knn_labels.append(vote)\n",
    "    \n",
    "\n",
    "    return knn_labels\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, **KNN**, which takes as input the training data (`trainx` and `trainy`), the test points (`evalx`), the value of **K** (integer), and a parameter for deciding the distance metric to be used (for example 1 for L1 and 2 for L2) and predicts labels for these test points using KNN classification. These labels should be returned in a `numpy` array with one entry per test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(trainx, trainy, evalx, K, dist_metric=2):\n",
    "    # output: an np.array of the predicted values for testy \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "        knn_labels = []\n",
    "    \n",
    "        \n",
    "        for points in evalx:\n",
    "            if dist_metric == 1:\n",
    "                distances = [l1_distance(x,points) for x in trainx]\n",
    "            else:\n",
    "                distances = [l2_distance(x,points) for x in trainx]\n",
    "\n",
    "            knn = np.argsort(distances)[:k]\n",
    "\n",
    "            labels = trainy[knn]\n",
    "\n",
    "            vote = mode(labels)\n",
    "   \n",
    "            knn_labels.append(vote)\n",
    "    \n",
    "\n",
    "        return knn_labels\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that allows you to select the hyper-parameters (distance measure and the value of K) by calling the KNN classifier with different values of K and either L1 or L2 distance measure. Make sure that you set the hyper-parameters using the validation set and not the test set. You need to systemtically try different values for K in conjunction with a distance measure and tabulate the results (you can do that be craeting a seperate cell and documenting in that cell) and note down the best hyper-parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NN with L2 distance accuracy =  0.8953488372093024\n",
      "KNN with L2 distance accuracy =  0.9534883720930233\n",
      " NN with L1 distance accuracy =  0.9418604651162791\n",
      "KNN with L1 distance accuracy =  0.9651162790697675\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "nn_predict_l2 = NN_L2(trainx, trainy, valx)\n",
    "knn_predict_l2 = KNN_L2(trainx, trainy, valx,9)\n",
    "nn_predict_l1 = NN_L1(trainx, trainy, valx)\n",
    "knn_predict_l1 = KNN_L1(trainx, trainy, valx, 9)\n",
    "\n",
    "print(' NN with L2 distance accuracy = ',accuracy_score(valy,nn_predict_l2))\n",
    "\n",
    "print('KNN with L2 distance accuracy = ',accuracy_score(valy,knn_predict_l2))\n",
    "\n",
    "print(' NN with L1 distance accuracy = ',accuracy_score(valy,nn_predict_l1))\n",
    "\n",
    "print('KNN with L1 distance accuracy = ',accuracy_score(valy,knn_predict_l1))\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test errors and the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the hyper-parameters have been selected, we now would like to perform a final evaluation on the test set and record the error rates. Also, Write a function, **confusion**, which takes as input the true labels for the test set (that is, `testy`) as well as the predicted labels and returns the confusion matrix. The confusion matrix should be a `np.array` of shape `(10,10)`. Also, record the confusion matrix in your assignment report.\n",
    "\n",
    "**Note:** Record the cpu time it takes to perform the evaluation on the test set using functions like **time.time()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(testy, testy_fit):\n",
    "    # inputs: the correct labels, the fitted KNN labels \n",
    "    # output: a 10x10 np.array representing the confusion matrix as above\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    return confusion_matrix(testy,testy_fit)\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NN with L2 distance accuracy =  0.8941176470588236\n",
      "KNN with L2 distance accuracy =  0.9176470588235294\n",
      " NN with L1 distance accuracy =  0.9176470588235294\n",
      "KNN with L1 distance accuracy =  0.9176470588235294\n",
      " confusion matrix for NN with L2 distance accuracy \n",
      " [[23  5]\n",
      " [ 4 53]]\n",
      "confusion matrix for KNN with L2 distance accuracy \n",
      " [[22  6]\n",
      " [ 1 56]]\n",
      " confusion matrix for NN with L1 distance accuracy \n",
      " [[23  5]\n",
      " [ 2 55]]\n",
      "confusion matrix for KNN with L1 distance accuracy \n",
      " [[22  6]\n",
      " [ 1 56]]\n"
     ]
    }
   ],
   "source": [
    "# Code for performing the final evaluation on the test set and generating the confuson matrix.\n",
    "### START CODE HERE ###\n",
    "nn_predict_l2 = NN_L2(trainx, trainy, testx)\n",
    "knn_predict_l2 = KNN_L2(trainx, trainy, testx,9)\n",
    "nn_predict_l1 = NN_L1(trainx, trainy, testx)\n",
    "knn_predict_l1 = KNN_L1(trainx, trainy, testx, 9)\n",
    "\n",
    "print(' NN with L2 distance accuracy = ',accuracy_score(testy,nn_predict_l2))\n",
    "\n",
    "print('KNN with L2 distance accuracy = ',accuracy_score(testy,knn_predict_l2))\n",
    "\n",
    "print(' NN with L1 distance accuracy = ',accuracy_score(testy,nn_predict_l1))\n",
    "\n",
    "print('KNN with L1 distance accuracy = ',accuracy_score(testy,knn_predict_l1))\n",
    "\n",
    "print(' confusion matrix for NN with L2 distance accuracy \\n', confusion(testy, nn_predict_l2))\n",
    "\n",
    "print('confusion matrix for KNN with L2 distance accuracy \\n', confusion(testy, knn_predict_l2))\n",
    "\n",
    "print(' confusion matrix for NN with L1 distance accuracy \\n', confusion(testy, nn_predict_l1))\n",
    "\n",
    "print('confusion matrix for KNN with L1 distance accuracy \\n', confusion(testy, knn_predict_l1))\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preparing the assignment report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to prepare the assignment report and submit your code and report to the Blackboard assignment. You need to record your answers for the following questions in the report:\n",
    "1. What is the error rate on the validation set for NN_L2?\n",
    "2. What is the best error rate on the validation set for KNN_L2?\n",
    "3. What is the error rate on the validation set for NN_L1?\n",
    "4. What is the best error rate on the validation set for KNN_L1?\n",
    "5. What is the error rate on the test set?\n",
    "7. Do you need to normalize data, in general, when using KNN?\n",
    "\n",
    "8. Do you need to normalize data when using KNN for the given problem? Explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Extra Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are invited to try some more ideas as extra work like:\n",
    "1. Implementing weighted KNN where the vote of a neighbour is scaled down based on its distance from the test point.\n",
    "2. Implement L_infinity distance measure and use it for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to keep in mind the following points:\n",
    "1. Use numpy arrays and numpy libraries for efficient computations. \n",
    "2. Vectorize the code wherever possible instead of using explicit loops. This will significantly speed-up your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
