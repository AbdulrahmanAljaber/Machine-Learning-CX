{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Prediction\n",
    "In this assignment we are going to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). Cover_Type (7 types, integer 1 to 7). The seven types are:\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "\"Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).\" [https://archive.ics.uci.edu/ml/datasets/covertype] \n",
    "\n",
    "In order to classify the forest cover, we will use several different classifiers and compare their results. The classifiers we will use are Decision Trees, Bagging, Boosting, and Random Forest. In this assignment you are suppose to use built-in classifiers from `sklearn`. The training, validation, and test partitions are provided. You may need to do some preprocessing, and of course hyper-parameter tuning for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype = datasets.fetch_covtype()\n",
    "X = covtype.data\n",
    "Y = covtype.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((581012, 54), (581012,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "perm = np.random.permutation(581012)\n",
    "trainx = X[perm[0:49500],:]\n",
    "trainy = Y[perm[0:49500]]\n",
    "valx = X[perm[49500:55000],:]\n",
    "valy = Y[perm[49500:55000]]\n",
    "testx = X[perm[55000:581012],:]\n",
    "testy = Y[perm[55000:581012]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17945, 24251, 3023, 254, 786, 1481, 1760)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(trainy==1), sum(trainy==2), sum(trainy==3), sum(trainy==4), sum(trainy==5), sum(trainy==6), sum(trainy==7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# training and hyper-parameter tuning\n",
    "tree_parameters = {'criterion':['gini','entropy'],'max_depth':np.arange(1,10)}\n",
    "\n",
    "grid_tree = GridSearchCV(estimator= DecisionTreeClassifier(),\n",
    "                        param_grid = tree_parameters)\n",
    "grid_tree.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters {'criterion': 'gini', 'max_depth': 9}\n",
      "Best accuracy : 0.7526666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyper-parameters\",grid_tree.best_params_)\n",
    "print(\"Best accuracy :\",grid_tree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7572727272727273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = grid_tree.predict(valx)\n",
    "print(\"Accuracy:\",accuracy_score(valy, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7537774803616647\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "y_pred = grid_tree.predict(testx)\n",
    "print(\"Accuracy:\",accuracy_score(testy, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
       "                                         max_features=0.5, n_estimators=100),\n",
       "             param_grid={'base_estimator__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         'max_samples': [0.05, 0.1, 0.2, 0.5]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "# training and hyper-parameter tuning\n",
    "param_grid = {\n",
    "    'base_estimator__max_depth' : np.arange(1,10),\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "grid_bag = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                     n_estimators = 100, max_features = 0.5),\n",
    "                                     param_grid)\n",
    "grid_bag.fit(trainx, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters {'base_estimator__max_depth': 9, 'max_samples': 0.5}\n",
      "Best accuracy : 0.7667070707070707\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyper-parameters\",grid_bag.best_params_)\n",
    "print(\"Best accuracy :\",grid_bag.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7665454545454545\n"
     ]
    }
   ],
   "source": [
    "#val\n",
    "y_pred = grid_bag.predict(valx)\n",
    "print(\"Accuracy:\",accuracy_score(valy, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7706135981688631\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "y_pred = grid_bag.predict(testx)\n",
    "print(\"Accuracy:\",accuracy_score(testy, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# training and hyper-parameter tuning\n",
    "param_grid = {\n",
    "    'base_estimator__max_depth' : np.arange(1,10),\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "    'learning_rate':[0.1,0.2,0.01,0.001,0.0001]\n",
    "    'n_estimators': [100 ,200 ,50]\n",
    "}\n",
    "\n",
    "grid_boost = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(),param_grid)\n",
    "grid_boost.fit(trainx, trainy)\n",
    "\n",
    "\n",
    "# ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth =1) n estimators = 200 , algorithm = 'SAMME.R',learning_rate = 0.1)\n",
    "# grid_boost.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyper-parameters\",grid_boost.best_params_)\n",
    "print(\"Best accuracy :\",grid_boost.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val\n",
    "y_pred = grid_boost.predict(valx)\n",
    "print(\"Accuracy:\",accuracy_score(valy, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "y_pred = grid_boost.predict(testx)\n",
    "print(\"Accuracy:\",accuracy_score(testy, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# training and hyper-parameter tuning\n",
    "param_grid = { \n",
    "    'n_estimators': np.arange(100,1000,100),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_rand = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "grid_rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyper-parameters\",grid_rand.best_params_)\n",
    "print(\"Best accuracy :\",grid_rand.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val\n",
    "y_pred = grid_rand.predict(valx)\n",
    "print(\"Accuracy:\",accuracy_score(valy, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "y_pred = grid_rand.predict(testx)\n",
    "print(\"Accuracy:\",accuracy_score(testy, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "Please report in you submission the following for each classifier:\n",
    "1. The best result on the validaton set\n",
    "2. Hyperparameter values for the classifier\n",
    "3. The result on the test set.\n",
    "\n",
    "Apart from the above, please provide your comments and observations on the results of the different classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Statlog (Vehicle Silhouettes) Data Set (https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29)\n",
    "\n",
    "It has 18 features and following four classes:\n",
    "**OPEL, SAAB, BUS, VAN**\n",
    "\n",
    "The purpose is to classify a given silhouette as one of four types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Decision Tree classifer to train the classifier with train/val/test partition as 70/15/15 (random seed=777)\n",
    "Fine-tune the tree on the validation set.\n",
    "\n",
    "1. Report the performance on the test set\n",
    "2. Extract the decision rules for classifiecation.\n",
    "3. Plot the decision tree as an image using some library.\n",
    "\n",
    "**Note:** You have library support in Python for visualizing the learnt decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
